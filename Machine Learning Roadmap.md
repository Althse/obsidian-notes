# Roadmap matematyki do zrozumienia Machine Learningu

## 🎯 Cel

Zbudowanie solidnych podstaw matematycznych do zrozumienia działania algorytmów ML (szczególnie regresji, klasyfikacji, klasteryzacji itp.).

## 🔢 Etap 1: Arytmetyka i podstawy algebry

### Zakres:

- Własności liczb rzeczywistych
    
- Potęgi, pierwiastki, logarytmy
    
- Układy równań liniowych
    
- Przekształcenia algebraiczne
    

### Materiały:

- 📘 _Khan Academy: Algebra 1, Algebra 2_ (darmowe)
    
- 📗 Książka: "Algebra liniowa w zadaniach" – A. Białynicki-Birula (polski klasyk)
    
- 📘 Książka: "No Bullshit Guide to Math and Physics" – Ivan Savov (świetny dla samouków)

- 📗 Książka: Matematyka w uczeniu maszynowym - Marc Peter Deisenroth
    

---

## 🔣 Etap 2: Algebra liniowa

### Zakres:

- Wektory i operacje na wektorach
    
- Iloczyn skalarny i wektorowy
    
- Macierze, działania na macierzach
    
- Rząd, wyznacznik, macierz odwrotna
    
- Układy równań liniowych jako macierze
    
- Przestrzenie liniowe, baza, wymiar
    
- Wartości własne i wektory własne (eigenvalues, eigenvectors)
    

### Materiały:

- 📘 _3Blue1Brown – Essence of Linear Algebra_ (YouTube)
    
- 📗 Książka: "Introduction to Linear Algebra" – Gilbert Strang
    
- 📘 Kurs MIT: Linear Algebra – Gilbert Strang (wideo + PDF)
    
- 🧮 Biblioteka do ćwiczeń: NumPy – operacje macierzowe, wektory
    

---

## 📈 Etap 3: Statystyka i prawdopodobieństwo

### Zakres:

- Średnia, mediana, wariancja, odchylenie standardowe
    
- Rozkład normalny, rozkłady prawdopodobieństwa
    
- Prawdopodobieństwo warunkowe, reguła Bayesa
    
- Zależność, korelacja, kowariancja
    
- Wnioskowanie statystyczne
    
- Pojęcia błędu: overfitting, underfitting (intuicyjnie)
    

### Materiały:

- 📘 _Khan Academy: Statistics and Probability_ (darmowe)
    
- 📗 Książka: "Naked Statistics" – Charles Wheelan (dla laików)
    
- 📗 Książka: "Think Stats" – Allen B. Downey (dostępna też za darmo)
    
- 📘 Kurs Harvard: Stat 110 – Probability
    

---

## 🔄 Etap 4: Rachunek różniczkowy i całkowy

### Zakres:

- Pochodne funkcji jednej zmiennej
    
- Reguły różniczkowania
    
- Ekstrema funkcji (maksimum/minimum)
    
- Całki oznaczone i nieoznaczone
    
- Pochodne cząstkowe i gradient
    

### Materiały:

- 📘 _Khan Academy: Calculus 1, 2, 3_ (pełny kurs)
    
- 📗 Książka: "Calculus, Early Transcendentals" – Stewart
    
- 📘 Kurs MIT OCW: Single Variable Calculus
    
- 📘 YouTube: _Essence of Calculus_ (3Blue1Brown)
    

---

## 📐 Etap 5: Optymalizacja

### Zakres:

- Funkcje wypukłe/wklęsłe
    
- Metoda gradientu (gradient descent)
    
- Rola funkcji kosztu / loss function
    
- Minimum lokalne a minimum globalne
    
- Reguły uczenia: learning rate, momentum, itp.
    

### Materiały:

- 📘 _DeepLizard: Gradient Descent and Backpropagation_ (YouTube)
    
- 📗 Książka: "Convex Optimization" – Stephen Boyd, Lieven Vandenberghe (zaawansowane)
    
- 📘 Blog: Distill.pub – bardzo intuicyjne wizualizacje
    

---

## 🧠 BONUS: Praktyka Machine Learning (dla zrozumienia zastosowań)

### Polecane źródła:

- 📘 Kurs Andrew Ng (Coursera): _Machine Learning_ (klasyk, podstawy regresji i klasyfikacji)
    
- 📗 Książka: "Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow" – Aurélien Géron
    
- 📘 Kurs Google: Machine Learning Crash Course
    
- 📘 Książka: "Python Data Science Handbook" – Jake VanderPlas (świetna praktyka z NumPy, pandas, matplotlib, scikit-learn)